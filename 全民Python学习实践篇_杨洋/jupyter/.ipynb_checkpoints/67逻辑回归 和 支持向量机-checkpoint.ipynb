{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4de25c17",
   "metadata": {},
   "source": [
    "###     读取红酒指标数据，使用逻辑回归求测试集的准确率，对以下未知分类数据进行预测：\n",
    "\n",
    "    [15.48, 1.8, 2.64, 20.5, 70.0, 2.60, 1.10, 0.52, 2.29, 10, 0.57, 1.78, 900.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "589a42a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>'酒精'</th>\n",
       "      <th>'苹果酸'</th>\n",
       "      <th>'灰'</th>\n",
       "      <th>'灰的碱性'</th>\n",
       "      <th>'镁'</th>\n",
       "      <th>'总酚'</th>\n",
       "      <th>'类黄酮'</th>\n",
       "      <th>'非黄烷类酚类'</th>\n",
       "      <th>'花青素'</th>\n",
       "      <th>'颜色强度'</th>\n",
       "      <th>'色调'</th>\n",
       "      <th>'od280/od315'</th>\n",
       "      <th>'脯氨酸'</th>\n",
       "      <th>分类</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      '酒精'  '苹果酸'   '灰'  '灰的碱性'    '镁'  '总酚'  '类黄酮'  '非黄烷类酚类'  '花青素'  '颜色强度'  \\\n",
       "0    14.23   1.71  2.43    15.6  127.0  2.80   3.06      0.28   2.29    5.64   \n",
       "1    13.20   1.78  2.14    11.2  100.0  2.65   2.76      0.26   1.28    4.38   \n",
       "2    13.16   2.36  2.67    18.6  101.0  2.80   3.24      0.30   2.81    5.68   \n",
       "3    14.37   1.95  2.50    16.8  113.0  3.85   3.49      0.24   2.18    7.80   \n",
       "4    13.24   2.59  2.87    21.0  118.0  2.80   2.69      0.39   1.82    4.32   \n",
       "..     ...    ...   ...     ...    ...   ...    ...       ...    ...     ...   \n",
       "173  13.71   5.65  2.45    20.5   95.0  1.68   0.61      0.52   1.06    7.70   \n",
       "174  13.40   3.91  2.48    23.0  102.0  1.80   0.75      0.43   1.41    7.30   \n",
       "175  13.27   4.28  2.26    20.0  120.0  1.59   0.69      0.43   1.35   10.20   \n",
       "176  13.17   2.59  2.37    20.0  120.0  1.65   0.68      0.53   1.46    9.30   \n",
       "177  14.13   4.10  2.74    24.5   96.0  2.05   0.76      0.56   1.35    9.20   \n",
       "\n",
       "     '色调'  'od280/od315'   '脯氨酸'  分类  \n",
       "0    1.04           3.92  1065.0   0  \n",
       "1    1.05           3.40  1050.0   0  \n",
       "2    1.03           3.17  1185.0   0  \n",
       "3    0.86           3.45  1480.0   0  \n",
       "4    1.04           2.93   735.0   0  \n",
       "..    ...            ...     ...  ..  \n",
       "173  0.64           1.74   740.0   2  \n",
       "174  0.70           1.56   750.0   2  \n",
       "175  0.59           1.56   835.0   2  \n",
       "176  0.60           1.62   840.0   2  \n",
       "177  0.61           1.60   560.0   2  \n",
       "\n",
       "[178 rows x 14 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 20)\n",
    "sh=pd.read_csv(r'C:\\Users\\YcAllenEffy\\Desktop\\红酒.csv')\n",
    "df=sh.copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c527dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7a33cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#划分训练,测试集\n",
    "df_train,df_test =train_test_split(df,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15743eb2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(solver='newton-cg')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#训练模型\n",
    "model= LogisticRegression(solver='newton-cg')\n",
    "model.fit(df_train.iloc[:,:-1],df_train['分类'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abc26d3",
   "metadata": {},
   "source": [
    "#### solver参数说明:\n",
    "sklearn为我们提供了多种求解逻辑回归参数θ 的方法（梯度下降法是其中著名的一种），让我们可以使用不同的求解器来计算逻辑回归。求解器的选择，由参数\"solver\"控制，共有五种选择。其中“liblinear”是二分类专用，也是现在的默认求解器。\\\n",
    "\n",
    "liblinear : 坐标下降法\\\n",
    "lbfgs：拟牛顿法的一种，利用损失函数二阶导数矩阵（海森矩阵）来迭代优化损失函数。\\\n",
    "newton-cg : 牛顿法的一种，利用损失函数二阶导数矩阵（海森矩阵）来迭代优化损失函数。\\\n",
    "sag ：随机平均梯度下降，与普通梯度下降法的区别是每次迭代仅仅用一部分的样本来计算梯度。\\\n",
    "saga：随机平均梯度下降的进化，稀疏多项逻辑回归的首选。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c65aeda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#打分\n",
    "y_predict=model.predict(df_test.iloc[:,:-1])\n",
    "accuracy_score(df_test['分类'],y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3e78473",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\CODE\\Anaconda\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[15.48, 1.8, 2.64, 20.5, 70.0, 2.60, 1.10, 0.52, 2.29, 10, 0.57, 1.78, 900.0]\n",
    "ans_predict=model.predict([a,])\n",
    "ans_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69753308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率： 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\CODE\\Anaconda\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 标准答案\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 读取数据\n",
    "df = sh.copy()\n",
    "\n",
    "# 拆分测试集与训练集\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split( df, random_state=42 )\n",
    "\n",
    "# 逻辑回归训练\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(solver='newton-cg')\n",
    "model.fit( df_train.iloc[:, 0:13], df_train['分类'] )\n",
    "\n",
    "# 预测测试集\n",
    "y_predict = model.predict( df_test.iloc[:, 0:13] )\n",
    "\n",
    "# 计算准确性\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('准确率：', accuracy_score( df_test['分类'], y_predict ))\n",
    "\n",
    "# 预测未知分类\n",
    "unknown = [15.48, 1.8, 2.64, 20.5, 70.0, 2.60, 1.10, 0.52, 2.29, 10, 0.57, 1.78, 900.0]\n",
    "unknown_predict = model.predict([unknown])\n",
    "unknown_predict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
